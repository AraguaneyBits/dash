# errata:
# - sudo/dist/group are set so as to get Blue Box VMs, necessary for [loopback]
#   IPv6 support

sudo: required
dist: trusty

os: linux
language: generic

addons:
  apt:
    packages:
      # Use more recent docker version
      - docker-ce
      # Can be removed if Travis ever upgrades to Bionic
      - realpath
      - python3
      - python3-pip

services:
  - docker

cache:
  apt: true
  ccache: true
  directories:
    - $HOME/cache

env:
  global:
    # DOCKER_HUB_USER
    - secure: "J0T+zrRBBzBnxoioCD378cRSJUF8Vms32QMmrENVVfuJl03oBQv0QxSatLEiD4PtRPrN00mJ8/k/HdqFQ0SN1yg6a00duOZ6C9jk6t/fExBO1LNhp3Z7lcaMUrNEyRN6sgfVeznDVTGj9jBQU2HNlwHk0UD9BNp/k+Bjhrtw7Njp9JTLZKBgxrXk0WZmfk0d75Q+9DajA1j2tSfWVpwzv6HMGfqIH1wsVEakBY+tnaE6EO3IdzdWHhf/iV+Tx0tL0Qz/6mejgSuoGLTJLmeTrceTTmsDlN3B95y+XfcYDs6EP7gpzN8Ffg/mLV8xJ2Myj7+b1o+smMJZE5tLuPIeAu0YsfkPNa/tNLotOlDxF72per0kXyNYbRf+JrpxiOu9pLCtdtd40V2SsiNhpQqHVy+b+wkkqo2syTHT+9UNkqEi3WFm6TqFuOeQNtDm6R0hRl/Talw/ZEeKs68z1Cf6VyBrZG5LqtOurtWlKwXCe/Tr1i2g4BEAcbywphtCCcfgMK9YSGQnjKkGk6k9gVEa2bneYLZz1RSh2OpXnIsIJOX7X0hNd5xtQDLgbpSUlKwVcEriWeGe12GCszKNX9gdEo2bHeq4xAJ6BE+ot745z9dc+nC2BJ6Su+I5L6zBg4m2FWn0TWhPvz/6JR26t6ls/E7m+hkwJTCHnIUiFmmzpEw="
    # DOCKER_HUB_PASSWORD
    - secure: "RLzlMhfLqwSBrZqJOVOd61suXBn+HtUR3vOZfuFYF/Qmjjj5TE41+rObZmzc54hP/ZL+OH6blnibpvfDXlX+eN38ivFQfuxkJIGL68SJsEwNBRwW39Yw6Hl5RdI41MLCH7ByN15wifLp1JKBilHJ7XGMOUjI7P0yl7JjX8GBXUCtJbVLRugo80/yn+XQ1NdnlpbpYHNjMEQFWSODPa3pSK5McWvyQjDZDgS+IkdrZmIYJPMa7bmKH5I/edUPSmXQT905FgEwq9u8XR0SyBopli21EK9l6GkXIIvmDTYz5vT26Apvi2B4Aoazlklg+KNRUJuLGerpt6kbnU0gMSUChVkFfFhOk6GRSN3a/AUfD2FOudvMhet2QvlPHx+GYdEUr5XVo5HW42pHsqfD6eDtHd8VLTsHP0q4C8V85fNMv21lkkehy2ry8fx/RRy6x4O2wg2mua+79UkGKXp75gMKzWEcth34PCFCOu37l2F8R/ANnrQ52K/8vIQ88TtU2OpYX89fHjLojBxu+WKEBGZH2LRPsZBOUHeeO5C/xKDrhZU24ORnMW8wg66Qg5GIX1KI4a8yp73Mpues5hzpJ6wkMuRrQt40ymKndLCjv8KSd+5BfP6Or/KIrzDNYdZaasjk7JNi6rcZmm9d3fTAo+Ja/mjpUCIOo3SX14luzVCJIig="
    - DOCKER_BUILD=false
    - S3_BUCKET=s3://dashevo-travis-shared-data
    - S3_BASE=s3://dashevo-travis-shared-data/$TRAVIS_BUILD_NUMBER

before_install:
  # set up awscli packages
  - pip3 install --user awscli

install:
  # Our scripts try to be Travis agnostic
  - export PULL_REQUEST="$TRAVIS_PULL_REQUEST"
  - export BUILD_NUMBER="$TRAVIS_BUILD_NUMBER"
  - export HOST_SRC_DIR=$TRAVIS_BUILD_DIR
  - export HOST_CACHE_DIR=$HOME/cache
  - source ./ci/matrix.sh
  - mkdir -p $HOST_CACHE_DIR/docker && mkdir -p $HOST_CACHE_DIR/ccache && mkdir -p $HOST_CACHE_DIR/depends && mkdir -p $HOST_CACHE_DIR/sdk-sources
  # Keep this as it makes caching related debugging easier
  - ls -lah $HOST_CACHE_DIR && ls -lah $HOST_CACHE_DIR/depends && ls -lah $HOST_CACHE_DIR/ccache && ls -lah $HOST_CACHE_DIR/docker

before_script:
  # Make sure stdout is in blocking mode. Otherwise builds will fail due to large writes to stdout
  # See https://github.com/travis-ci/travis-ci/issues/4704. If this gets fixed, this line can also be removed.
  - python3 -c 'import os,sys,fcntl; flags = fcntl.fcntl(sys.stdout, fcntl.F_GETFL); fcntl.fcntl(sys.stdout, fcntl.F_SETFL, flags&~os.O_NONBLOCK);'
  - export TRAVIS_COMMIT_LOG=`git log --format=fuller -1`
  - echo TRAVIS_BUILD_STAGE_NAME=$TRAVIS_BUILD_STAGE_NAME
  - echo DASH_BUILD_STAGE=$DASH_BUILD_STAGE

  # If requested, wait for stage success/failed markers to appear
  - |
    if [ "$WAIT_FOR_STAGE" != "" ]; then
      ABORT=false
      while true; do
        if aws s3 ls $S3_BASE/$BUILD_TARGET/$WAIT_FOR_STAGE/stage-success >& /dev/null; then
          echo "Stage $WAIT_FOR_STAGE has succeeded, continuing"
          break
        fi
        if aws s3 ls $S3_BASE/$BUILD_TARGET/$WAIT_FOR_STAGE/stage-failed >& /dev/null; then
          echo "Stage $WAIT_FOR_STAGE has failed, aborting"
          ABORT=true
          break
        fi
        echo "Waiting for stage $WAIT_FOR_STAGE to finish..."
        sleep 5
      done
      # Indicate failure to Travis
      if [ "$ABORT" = "true" ]; then false; fi
    fi

  #### BEGIN build stage ####
  # Load builder image cache
  - |
    if [ "$DASH_BUILD_STAGE" = "Build" -a -f $HOST_CACHE_DIR/docker/dash-builder-$BUILD_TARGET.tar.gz ]; then
      zcat $HOST_CACHE_DIR/docker/dash-builder-$BUILD_TARGET.tar.gz | docker load || true
    fi
  #### END build stage ####

  #### BEGIN tests and upload-docker stage ####
  # Load builder image from S3
  - |
    if [ "$DASH_BUILD_STAGE" != "Build" ]; then
      echo "Downloading builder image..."
      aws s3 cp $S3_BASE/$BUILD_TARGET/dash-builder-$BUILD_TARGET.tar.gz ./dash-builder-$BUILD_TARGET.tar.gz
      zcat dash-builder-$BUILD_TARGET.tar.gz | docker load
    fi
  # Load build directory (with binaries) from S3
  - if [ "$DASH_BUILD_STAGE" = "Tests" -o "$DASH_BUILD_STAGE" = "Upload-docker" ]; then aws s3 cp $S3_BASE/$BUILD_TARGET/build-ci.tar.gz ./build-ci.tar.gz && tar -xzf build-ci.tar.gz && rm build-ci.tar.gz; fi
  #### END tests and upload-docker stage ####

script:
  #### BEGIN build stage ####
  # Build builder image
  - if [ "$DASH_BUILD_STAGE" = "Build" ]; then travis_retry docker build --pull -t $BUILDER_IMAGE_NAME --build-arg=USER_ID=$UID --build-arg=GROUP_ID=$UID --build-arg=BUILD_TARGET=$BUILD_TARGET -f ci/Dockerfile.builder ci; fi

  # Build depends
  - if [ "$DASH_BUILD_STAGE" = "Build" ]; then $DOCKER_RUN_IN_BUILDER ./ci/build_depends.sh; fi
  # Gracefully stop build without running into timeouts (which won't update caches) when building depends took too long
  # Next build should fix this situation as it will start with a populated depends cache
  - if [ $SECONDS -gt 1200 ]; then export DEPENDS_TIMEOUT="true"; false; fi # The "false" here ensures that the build is marked as failed even though the whole script returns 0

  # Build src
  - test "$DEPENDS_TIMEOUT" != "true" && if [ "$DASH_BUILD_STAGE" = "Build" ]; then $DOCKER_RUN_IN_BUILDER ./ci/build_src.sh; fi
  # Unit tests (need to be run in build stage)
  - test "$DEPENDS_TIMEOUT" != "true" && if [ "$DASH_BUILD_STAGE" = "Build" ]; then $DOCKER_RUN_IN_BUILDER ./ci/test_unittests.sh; fi
  #### END build stage ####

  #### BEGIN tests stage ####
  # Integration tests
  - if [ "$DASH_BUILD_STAGE" = "Tests" ]; then echo RPC_TEST_ARGS=$RPC_TEST_ARGS && $DOCKER_RUN_IN_BUILDER ./ci/test_integrationtests.sh $RPC_TEST_ARGS; fi
  #### END tests stage ####

  #### BEGIN upload-docker stage ####
  - if [ "$DASH_BUILD_STAGE" = "Upload-docker" ]; then BUILD_DIR=build-ci/dashcore-$BUILD_TARGET ./docker/build-docker.sh; fi
  #### END upload-docker stage ####

before_cache:
  #### BEGIN build stage ####
  # Save builder image to cache
  - |
    if [ "$DASH_BUILD_STAGE" = "Build" ]; then
      SAVE_ARGS="$(docker history -q dash-builder-$BUILD_TARGET | grep -v \<missing\>)"
      echo "Saving builder image..."
      docker save dash-builder-$BUILD_TARGET $SAVE_ARGS | gzip -2 > $HOME/cache/docker/dash-builder-$BUILD_TARGET.tar.gz
    fi
  #### END build stage ####

after_success:
  #### BEGIN build stage ####
  # Upload builder image to S3 (needed by Tests/Upload-docker stage)
  - |
    if [ "$DASH_BUILD_STAGE" = "Build" ]; then
      echo "Copying builder image to S3..."
      aws s3 cp $HOME/cache/docker/dash-builder-$BUILD_TARGET.tar.gz $S3_BASE/$BUILD_TARGET/dash-builder-$BUILD_TARGET.tar.gz
    fi

  # Upload build dir (with binaries)
  - if [ "$DASH_BUILD_STAGE" = "Build" ]; then GZIP=-2 tar -czf build-ci.tar.gz build-ci && aws s3 cp ./build-ci.tar.gz $S3_BASE/$BUILD_TARGET/build-ci.tar.gz; fi
  #### END build stage ####

  #### BEGIN upload-docker stage ####
  - if [ "$DASH_BUILD_STAGE" = "Upload-docker" -a "$TRAVIS_BRANCH" = "develop" -a "$TRAVIS_PULL_REQUEST" = "false" ]; then docker login -u $DOCKER_HUB_USER -p $DOCKER_HUB_PASSWORD && ./docker/push-docker.sh; fi
  #### END upload-docker stage ####

  # Put stage success marker onto S3
  - touch /tmp/stage-success && aws s3 cp /tmp/stage-success $S3_BASE/$BUILD_TARGET/$DASH_BUILD_STAGE/stage-success

after_failure:
  # Put stage failure marker onto S3
  - touch /tmp/stage-failed && aws s3 cp /tmp/stage-failed $S3_BASE/$BUILD_TARGET/$DASH_BUILD_STAGE/stage-failed

jobs:
  include:
  #############
  - stage: Build and test
    name: "Build linux32"
    env:
    - BUILD_TARGET=linux32
    - DASH_BUILD_STAGE=Build
  - name: "Build linux64"
    env:
    - BUILD_TARGET=linux64
    - DASH_BUILD_STAGE=Build
  - name: "Build arm-linux"
    env:
    - BUILD_TARGET=arm-linux
    - DASH_BUILD_STAGE=Build
  - name: "Build win32"
    env:
    - BUILD_TARGET=win32
    - DASH_BUILD_STAGE=Build
  - name: "Build win64"
    env:
    - BUILD_TARGET=win64
    - DASH_BUILD_STAGE=Build
  - name: "Build linux64_nowallet"
    env:
    - BUILD_TARGET=linux64_nowallet
    - DASH_BUILD_STAGE=Build
  - name: "Build linux64_release"
    env:
    - BUILD_TARGET=linux64_release
    - DASH_BUILD_STAGE=Build
  - name: "Build mac"
    env:
    - BUILD_TARGET=mac
    - DASH_BUILD_STAGE=Build
  #############
  # Warning, these are scheduled in parallel to the build jobs above to utilize all workers in parallel.
  # We assume that in most cases the linux32 and linux64 builds finish faster then the other ones and at the same
  # time rely on the maximum number of parallel workers to be 5. As we have a total of 7 builds scheduled, we run
  # into a few problems when at least 4 of the other builds finish faster then linux32/linux64. If that happens,
  # the test jobs will wait for a marker file to appear on S3, so that they know that they can start doing their jobs.
  # This might run into timeouts in tests from time to time when the linux32/linux64 builds take too much time.
  # TODO run -extended tests as well (but these fail atm)
  - name: Integration Tests Linux32 Masternodes
    env:
    - BUILD_TARGET=linux32
    - DASH_BUILD_STAGE=Tests
    - RPC_TEST_ARGS="-masternodes -parallel=2"
    - WAIT_FOR_STAGE=Build
  - name: Integration Tests Linux64 Masternodes
    env:
    - BUILD_TARGET=linux64
    - DASH_BUILD_STAGE=Tests
    - RPC_TEST_ARGS="-masternodes -parallel=2"
    - WAIT_FOR_STAGE=Build
  - name: Integration Tests Linux32 Regular
    env:
    - BUILD_TARGET=linux32
    - DASH_BUILD_STAGE=Tests
    - RPC_TEST_ARGS="-regular -parallel=4"
    - WAIT_FOR_STAGE=Build
  - name: Integration Tests Linux64 Regular
    env:
    - BUILD_TARGET=linux64
    - DASH_BUILD_STAGE=Tests
    - RPC_TEST_ARGS="-regular -parallel=4"
    - WAIT_FOR_STAGE=Build

  #############
  - stage: Upload-docker
    after_script:
    # We reuse this stage to also cleanup the S3 bucket. The stage has only one job for one target, so this is fine
    - aws s3 rm --recursive $S3_BASE
    - |
      for i in $(aws s3 ls $S3_BUCKET/ | awk '{print $2}' | sed 's|/||g'); do
        # Delete stage storage for older builds
        if [ "$i" -lt "$(($TRAVIS_BUILD_NUMBER - 5))" ]; then
            aws s3 rm --recursive $S3_BUCKET/$i
        fi
      done
    env:
    - BUILD_TARGET=linux64_release
    - DASH_BUILD_STAGE=Upload-docker
